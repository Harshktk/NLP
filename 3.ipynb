{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf4347bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned and lemmatized text data:\n",
      "0    nltk leading platform building python program ...\n",
      "1    word2vec popular technique generating word emb...\n",
      "2                        stand term document frequency\n",
      "3    simple yet effective approach text representation\n",
      "4                                   apple orange fruit\n",
      "5                         banana good source potassium\n",
      "6                                      mango delicious\n",
      "Name: clean_text, dtype: object\n",
      "\n",
      "Label encoding mapping:\n",
      "{'technology': 1, 'fruits': 0}\n",
      "\n",
      "TF-IDF representation:\n",
      "     apple  approach  banana  building      data  delicious  document  \\\n",
      "0  0.00000  0.000000     0.0  0.316228  0.316228   0.000000       0.0   \n",
      "1  0.00000  0.000000     0.0  0.000000  0.000000   0.000000       0.0   \n",
      "2  0.00000  0.000000     0.0  0.000000  0.000000   0.000000       0.5   \n",
      "3  0.00000  0.408248     0.0  0.000000  0.000000   0.000000       0.0   \n",
      "4  0.57735  0.000000     0.0  0.000000  0.000000   0.000000       0.0   \n",
      "5  0.00000  0.000000     0.5  0.000000  0.000000   0.000000       0.0   \n",
      "6  0.00000  0.000000     0.0  0.000000  0.000000   0.707107       0.0   \n",
      "\n",
      "   effective  embeddings  frequency  ...    simple  source  stand  technique  \\\n",
      "0   0.000000    0.000000        0.0  ...  0.000000     0.0    0.0   0.000000   \n",
      "1   0.000000    0.408248        0.0  ...  0.000000     0.0    0.0   0.408248   \n",
      "2   0.000000    0.000000        0.5  ...  0.000000     0.0    0.5   0.000000   \n",
      "3   0.408248    0.000000        0.0  ...  0.408248     0.0    0.0   0.000000   \n",
      "4   0.000000    0.000000        0.0  ...  0.000000     0.0    0.0   0.000000   \n",
      "5   0.000000    0.000000        0.0  ...  0.000000     0.5    0.0   0.000000   \n",
      "6   0.000000    0.000000        0.0  ...  0.000000     0.0    0.0   0.000000   \n",
      "\n",
      "   term      text      word  word2vec      work       yet  \n",
      "0   0.0  0.000000  0.000000  0.000000  0.316228  0.000000  \n",
      "1   0.0  0.000000  0.408248  0.408248  0.000000  0.000000  \n",
      "2   0.5  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "3   0.0  0.408248  0.000000  0.000000  0.000000  0.408248  \n",
      "4   0.0  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "5   0.0  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "6   0.0  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "\n",
      "[7 rows x 35 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Snehal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Snehal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Snehal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "    {\"text\": \"NLTK is a leading platform for building Python programs to work with human language data.\", \"label\": \"technology\"},\n",
    "    {\"text\": \"Word2Vec is a popular technique for generating word embeddings.\", \"label\": \"technology\"},\n",
    "    {\"text\": \"TF-IDF stands for Term Frequency-Inverse Document Frequency.\", \"label\": \"technology\"},\n",
    "    {\"text\": \"Bag-of-Words is a simple yet effective approach for text representation.\", \"label\": \"technology\"},\n",
    "    {\"text\": \"Apples and oranges are fruits.\", \"label\": \"fruits\"},\n",
    "    {\"text\": \"Bananas are a good source of potassium.\", \"label\": \"fruits\"},\n",
    "    {\"text\": \"Mangoes are delicious.\", \"label\": \"fruits\"}\n",
    "]\n",
    "\n",
    "# Convert data to DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Text cleaning and lemmatization\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    clean_tokens = [lemmatizer.lemmatize(token) for token in tokens if token.isalnum() and token not in stop_words]\n",
    "    return \" \".join(clean_tokens)\n",
    "\n",
    "df['clean_text'] = df['text'].apply(clean_text)\n",
    "\n",
    "# Label encoding\n",
    "label_encoder = LabelEncoder()\n",
    "df['label_encoded'] = label_encoder.fit_transform(df['label'])\n",
    "\n",
    "# TF-IDF representation\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['clean_text'])\n",
    "\n",
    "# Save TF-IDF representation\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "tfidf_df.to_csv(\"tfidf_representation.csv\", index=False)\n",
    "\n",
    "# Show outputs\n",
    "print(\"Cleaned and lemmatized text data:\")\n",
    "print(df['clean_text'])\n",
    "print(\"\\nLabel encoding mapping:\")\n",
    "print(dict(zip(df['label'], df['label_encoded'])))\n",
    "print(\"\\nTF-IDF representation:\")\n",
    "print(tfidf_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae896d08",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239e8778",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

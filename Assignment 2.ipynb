{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ff08da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.Perform bag-of-words approach (count occurrence, normalized count occurrence), TF-IDF on data. Create embeddings using Word2Vec.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6483c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Snehal\\AppData\\Roaming\\nltk_data...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ef2f3be",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Snehal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Snehal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Snehal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    " # Sample data\n",
    "data = [\n",
    "    {\"text\": \"NLTK is a leading platform for building Python programs to work with human language data.\", \"label\": \"technology\"},\n",
    "    {\"text\": \"Word2Vec is a popular technique for generating word embeddings.\", \"label\": \"technology\"},\n",
    "    {\"text\": \"TF-IDF stands for Term Frequency-Inverse Document Frequency.\", \"label\": \"technology\"},\n",
    "    {\"text\": \"Bag-of-Words is a simple yet effective approach for text representation.\", \"label\": \"technology\"},\n",
    "    {\"text\": \"Apples and oranges are fruits.\", \"label\": \"fruits\"},\n",
    "    {\"text\": \"Bananas are a good source of potassium.\", \"label\": \"fruits\"},\n",
    "    {\"text\": \"Mangoes are delicious.\", \"label\": \"fruits\"}\n",
    "]\n",
    "\n",
    "# Convert data to DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Text cleaning and lemmatization\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    # Tokenize words\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    # Remove punctuation and stop words, lemmatize\n",
    "    clean_tokens = [lemmatizer.lemmatize(token) for token in tokens if token.isalnum() and token not in stop_words]\n",
    "    return \" \".join(clean_tokens)\n",
    "\n",
    "df['clean_text'] = df['text'].apply(clean_text)\n",
    "\n",
    "# TF-IDF representation\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['clean_text'])\n",
    "\n",
    "# Save TF-IDF representation\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "tfidf_df.to_csv(\"tfidf_representation.csv\", index=False)\n",
    "\n",
    "# Word2Vec\n",
    "word2vec_model = Word2Vec(sentences=[word_tokenize(text.lower()) for text in df['clean_text']], vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Save Word2Vec model\n",
    "word2vec_model.save(\"word2vec.model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "815e2f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF representation:\n",
      "     apple  approach  banana  building      data  delicious  document  \\\n",
      "0  0.00000  0.000000     0.0  0.316228  0.316228        0.0       0.0   \n",
      "1  0.00000  0.000000     0.0  0.000000  0.000000        0.0       0.0   \n",
      "2  0.00000  0.000000     0.0  0.000000  0.000000        0.0       0.5   \n",
      "3  0.00000  0.408248     0.0  0.000000  0.000000        0.0       0.0   \n",
      "4  0.57735  0.000000     0.0  0.000000  0.000000        0.0       0.0   \n",
      "\n",
      "   effective  embeddings  frequency  ...    simple  source  stand  technique  \\\n",
      "0   0.000000    0.000000        0.0  ...  0.000000     0.0    0.0   0.000000   \n",
      "1   0.000000    0.408248        0.0  ...  0.000000     0.0    0.0   0.408248   \n",
      "2   0.000000    0.000000        0.5  ...  0.000000     0.0    0.5   0.000000   \n",
      "3   0.408248    0.000000        0.0  ...  0.408248     0.0    0.0   0.000000   \n",
      "4   0.000000    0.000000        0.0  ...  0.000000     0.0    0.0   0.000000   \n",
      "\n",
      "   term      text      word  word2vec      work       yet  \n",
      "0   0.0  0.000000  0.000000  0.000000  0.316228  0.000000  \n",
      "1   0.0  0.000000  0.408248  0.408248  0.000000  0.000000  \n",
      "2   0.5  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "3   0.0  0.408248  0.000000  0.000000  0.000000  0.408248  \n",
      "4   0.0  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "\n",
      "[5 rows x 35 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load TF-IDF representation from the saved CSV file\n",
    "tfidf_df = pd.read_csv(\"tfidf_representation.csv\")\n",
    "\n",
    "# Print the first few rows of the TF-IDF dataframe\n",
    "print(\"TF-IDF representation:\")\n",
    "print(tfidf_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4305404d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word vector for 'word': [ 9.4563962e-05  3.0773198e-03 -6.8126451e-03 -1.3754654e-03\n",
      "  7.6685809e-03  7.3464094e-03 -3.6732971e-03  2.6427018e-03\n",
      " -8.3171297e-03  6.2054861e-03 -4.6373224e-03 -3.1641065e-03\n",
      "  9.3113566e-03  8.7338570e-04  7.4907029e-03 -6.0740625e-03\n",
      "  5.1605068e-03  9.9228229e-03 -8.4573915e-03 -5.1356913e-03\n",
      " -7.0648370e-03 -4.8626517e-03 -3.7785638e-03 -8.5361991e-03\n",
      "  7.9556061e-03 -4.8439382e-03  8.4236134e-03  5.2625705e-03\n",
      " -6.5500261e-03  3.9578713e-03  5.4701497e-03 -7.4265362e-03\n",
      " -7.4057197e-03 -2.4752307e-03 -8.6257253e-03 -1.5815723e-03\n",
      " -4.0343284e-04  3.2996845e-03  1.4418805e-03 -8.8142155e-04\n",
      " -5.5940580e-03  1.7303658e-03 -8.9737179e-04  6.7936908e-03\n",
      "  3.9735902e-03  4.5294715e-03  1.4343059e-03 -2.6998555e-03\n",
      " -4.3668128e-03 -1.0320747e-03  1.4370275e-03 -2.6460087e-03\n",
      " -7.0737829e-03 -7.8053069e-03 -9.1217868e-03 -5.9351693e-03\n",
      " -1.8474245e-03 -4.3238713e-03 -6.4606704e-03 -3.7173224e-03\n",
      "  4.2891586e-03 -3.7390434e-03  8.3781751e-03  1.5339935e-03\n",
      " -7.2423196e-03  9.4337985e-03  7.6312125e-03  5.4932819e-03\n",
      " -6.8488456e-03  5.8226790e-03  4.0090932e-03  5.1853694e-03\n",
      "  4.2559016e-03  1.9397545e-03 -3.1701624e-03  8.3538452e-03\n",
      "  9.6121803e-03  3.7926030e-03 -2.8369951e-03  7.1275235e-06\n",
      "  1.2188185e-03 -8.4583247e-03 -8.2239453e-03 -2.3101569e-04\n",
      "  1.2372875e-03 -5.7433806e-03 -4.7252737e-03 -7.3460746e-03\n",
      "  8.3286157e-03  1.2129784e-04 -4.5093987e-03  5.7017053e-03\n",
      "  9.1800150e-03 -4.0998720e-03  7.9646818e-03  5.3754342e-03\n",
      "  5.8791232e-03  5.1259040e-04  8.2130842e-03 -7.0190406e-03]\n"
     ]
    }
   ],
   "source": [
    "# Load the Word2Vec model\n",
    "word2vec_model = Word2Vec.load(\"word2vec.model\")\n",
    "\n",
    "# Retrieve the vector for a specific word (e.g., \"word\")\n",
    "word_vector = word2vec_model.wv[\"word\"]\n",
    "print(\"Word vector for 'word':\", word_vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7cc56b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
